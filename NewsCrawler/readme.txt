1.爬取的新闻存放在result文件夹中，一个新闻网页写入一个txt文档；
  txt文档内部为json格式数据，每个文档里面有4个字段：url、title、text和comment
  comment字段对应一个列表，列表中的每个元素为一条评论
  每条评论又各为一个字典，包含creatTime、username、content三个字段

2.爬虫中的path变量指定了爬取的新闻存放的位置；
  要对新闻进行更新时，需要将爬虫类中的start_urls中的url修改为新的种子url，个数不限。
  eg."http://news.163.com/17/1126/01/D44NK8R1000187VI.html"中17代表2017年，1126代表新闻日期11月26日

3.在Neteasenews1中的spiders文件夹中共有四只爬虫
  它们的代码和功能完全相同,只是为了同时运行加快爬取速度；

  在含有scrapy.cfg的目录（本目录）下运行"scrapy crawl netease1"即可启动爬虫1，
  同理启动其余三只爬虫。

4.爬虫开始爬取后，一开始速度很快，然后逐渐变慢最后稳定于一个速度；
  本次爬取共耗时约8.5h,爬取网页约10万个，平均每分钟爬取约200个网页（四只爬虫同步）。
  （加上评论的爬取之后可能会变慢于上述速度，因为对每个页面要去单独请求一次它对应的评论页面）

5.由于字典的无序性，导致写入文件时先写入了评论，正文标题等被压在了下面，
  暂时还未处理，不过在程序中应当不影响使用。